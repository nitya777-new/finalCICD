name: CI - Validate & Publish Model (safe)

on:
  push:
    branches: [ main ]

jobs:
  validate-and-publish:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      PYTHONUNBUFFERED: 1

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install apt deps
        run: |
          sudo apt-get update
          sudo apt-get install -y git-lfs

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python deps (CPU torch + requirements)
        env:
          PIP_DEFAULT_TIMEOUT: 120
        run: |
          python -m pip install --upgrade pip
          # Install CPU-only torch to avoid source builds
          python -m pip install --no-cache-dir "torch" --index-url https://download.pytorch.org/whl/cpu
          if [ -f requirements.txt ]; then
            python -m pip install --no-cache-dir -r requirements.txt
          else
            python -m pip install --no-cache-dir ultralytics opencv-python-headless mlflow "dvc[s3]" pyyaml
          fi

      - name: Configure DVC remote (DagsHub)
        env:
          DAGSHUB_REPO: ${{ secrets.DAGSHUB_REPO }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          # remove any old origin to avoid conflicts
          dvc remote remove origin 2>/dev/null || true
          dvc remote add -d origin s3://dvc
          dvc remote modify origin endpointurl https://dagshub.com/${DAGSHUB_REPO}.s3
          dvc remote modify origin --local access_key_id ${DAGSHUB_TOKEN}
          dvc remote modify origin --local secret_access_key ${DAGSHUB_TOKEN}
          dvc remote list || true
          sed -n '1,200p' .dvc/config.local || true

      - name: dvc pull (safe)
        run: |
          dvc pull || echo "no remote data to pull or pull failed"

      - name: Run evaluation
        run: |
          if [ -f "data/data.yaml" ]; then
            python evaluate.py --model models/best.pt --data data/data.yaml
          else
            echo "data/data.yaml not found; skipping eval."
          fi

      - name: Upload eval summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-summary
          path: eval_summary.json

      - name: dvc push (upload model blobs)
        env:
          DAGSHUB_REPO: ${{ secrets.DAGSHUB_REPO }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc push --jobs 4 || (echo "dvc push failed" && exit 1)
