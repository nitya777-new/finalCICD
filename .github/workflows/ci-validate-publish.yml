name: CI - Validate & Publish Model (safe)

on:
  push:
    branches: [ main ]

jobs:
  validate-and-publish:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      PYTHONUNBUFFERED: 1

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install apt deps
        run: |
          sudo apt-get update
          sudo apt-get install -y git-lfs

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python deps (CPU torch + requirements)
        env:
          PIP_DEFAULT_TIMEOUT: 120
        run: |
          python -m pip install --upgrade pip
          # Install CPU-only torch (explicit index avoids source builds)
          python -m pip install --no-cache-dir "torch" --index-url https://download.pytorch.org/whl/cpu
          if [ -f requirements.txt ]; then
            python -m pip install --no-cache-dir -r requirements.txt
          else
            python -m pip install --no-cache-dir ultralytics opencv-python-headless mlflow "dvc[s3]" pyyaml
          fi

      - name: Verify installs
        run: |
          python - <<'PY'
import importlib, sys
pkgs = ("torch","dvc","mlflow","ultralytics")
for p in pkgs:
    try:
        importlib.import_module(p)
        print(p, "ok")
    except Exception as e:
        print(p, "import failed:", e, file=sys.stderr)
PY

      - name: Configure DVC remote (DagsHub)  # <<--- FIXED STEP: builds URL variable and quotes it
        env:
          DAGSHUB_REPO: ${{ secrets.DAGSHUB_REPO }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          # remove any old origin to avoid conflicts
          dvc remote remove origin 2>/dev/null || true

          # add s3-style remote (canonical name)
          dvc remote add -d origin s3://dvc

          # build endpoint URL safely (ensures single argument)
          REPO="${DAGSHUB_REPO}"
          URL="https://dagshub.com/${REPO}.s3"

          # set the endpoint (quote the URL so it's passed as a single value)
          dvc remote modify origin endpointurl "$URL"

          # store token locally (not committed to repo)
          dvc remote modify origin --local access_key_id "${DAGSHUB_TOKEN}"
          dvc remote modify origin --local secret_access_key "${DAGSHUB_TOKEN}"

          # diagnostics for logs
          echo "=== dvc remote list ==="
          dvc remote list || true
          echo "=== .dvc/config.local preview ==="
          sed -n '1,200p' .dvc/config.local || true

      - name: dvc pull (safe)
        run: |
          dvc pull || echo "no remote data to pull or pull failed"

      - name: Run evaluation
        run: |
          if [ -f "data/data.yaml" ]; then
            python evaluate.py --model models/best.pt --data data/data.yaml
          else
            echo "data/data.yaml not found; skipping eval."
          fi

      - name: Upload eval summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-summary
          path: eval_summary.json

      - name: dvc push (upload model blobs)
        env:
          DAGSHUB_REPO: ${{ secrets.DAGSHUB_REPO }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc push --jobs 4 || (echo "dvc push failed" && exit 1)
