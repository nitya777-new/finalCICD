name: CI - Validate & Publish Model (safe)

on:
  push:
    branches: [ main ]

jobs:
  validate-and-publish:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    env:
      PYTHONUNBUFFERED: 1

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install apt deps
        run: |
          sudo apt-get update -y
          sudo apt-get install -y git-lfs

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install Python deps (CPU torch + requirements)
        env:
          PIP_DEFAULT_TIMEOUT: 120
        run: |
          python -m pip install --upgrade pip
          python -m pip install --no-cache-dir "torch" --index-url https://download.pytorch.org/whl/cpu
          if [ -f requirements.txt ]; then
            python -m pip install --no-cache-dir -r requirements.txt
          else
            python -m pip install --no-cache-dir ultralytics opencv-python-headless mlflow "dvc[s3]" pyyaml
          fi

      - name: Verify installs
        run: |
          python -c "import importlib,sys; print('torch:', __import__('torch').__version__ if importlib.util.find_spec('torch') else 'missing')"
          python -c "import importlib,sys; print('dvc ok' if importlib.util.find_spec('dvc') else 'dvc missing')"
          python -c "import importlib,sys; print('mlflow ok' if importlib.util.find_spec('mlflow') else 'mlflow missing')"
          python -c "import importlib,sys; print('ultralytics ok' if importlib.util.find_spec('ultralytics') else 'ultralytics missing')"

      - name: Configure DVC remote (DagsHub)
        env:
          DAGSHUB_REPO: ${{ secrets.DAGSHUB_REPO }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          # remove previous origin if any
          dvc remote remove origin 2>/dev/null || true

          # add canonical s3 remote name
          dvc remote add -d origin s3://dvc

          # build endpoint safely and quote it
          REPO="${DAGSHUB_REPO}"
          URL="https://dagshub.com/${REPO}.s3"

          dvc remote modify origin endpointurl "$URL"
          dvc remote modify origin --local access_key_id "${DAGSHUB_TOKEN}"
          dvc remote modify origin --local secret_access_key "${DAGSHUB_TOKEN}"

          echo "=== dvc remote list ==="
          dvc remote list || true
          echo "=== .dvc/config.local preview ==="
          sed -n '1,200p' .dvc/config.local || true

      - name: dvc pull (safe)
        run: |
          dvc pull || echo "no remote data to pull or pull failed"

      - name: Run evaluation
        run: |
          if [ -f "data/data.yaml" ]; then
            python evaluate.py --model models/best.pt --data data/data.yaml
          else
            echo "data/data.yaml not found; skipping eval."
          fi

      - name: Upload eval summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: eval-summary
          path: eval_summary.json

      - name: dvc push (upload model blobs)
        env:
          DAGSHUB_REPO: ${{ secrets.DAGSHUB_REPO }}
          DAGSHUB_TOKEN: ${{ secrets.DAGSHUB_TOKEN }}
        run: |
          dvc push --jobs 4 || (echo "dvc push failed" && exit 1)
